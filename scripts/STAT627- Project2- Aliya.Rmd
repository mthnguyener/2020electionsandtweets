---
title: "STAT627-Project2"
author: "Aliya Alimujiang"
date: "11/9/2020"
output: html_document
---

Presentation flow: 

Background on the data:
 
Variable selection: Backward(16),forward (21), best subset (16)

Model: 
GLM : 0.4 MSE
Classification tree: 

PCA:


Great interpretabiity but prediction accuracy was low. our goal was to have a high prediction accuracy. We opted run some models where we lose some interpretability but we will gain some prediction accuracy

RF: 
bagging: 

Pros and Cons. No idea how to explain it but good prediction with lowest MSE.


Run the new dataset on the final chosen model:


Conclusion: Talk about challenges and results

***


```{r}
library(tidyverse)
library(tree)
training <- read_csv("sent_set.csv")
```

## Data Clean-up:

```{r}
training%>%
   mutate(candidate=as.factor(case_when(candidate == "Biden" ~ 1 , candidate == "Trump" ~ 0)))%>%
   select(candidate, anticipation,sadness,fear,joy,positive,surprise,trust,anger,disgust,negative)->full.set

attach(full.set)

head(full.set)


```






#Variable selection

Best subset selection

```{r}
library(leaps)
reg.fit = regsubsets(candidate~.,nvmax=10, data=full.set)
summary(reg.fit)

plot( summary(reg.fit)$cp, main="Cp"); lines( summary(reg.fit)$cp )
plot( summary(reg.fit)$bic, main="BIC"); lines( summary(reg.fit)$bic )
plot( summary(reg.fit)$adjr2, main="ADJUSTED R-SQUARE"); lines( summary(reg.fit)$adjr2 )

which.min( summary(reg.fit)$cp )
which.min( summary(reg.fit)$bic )
which.max( summary(reg.fit)$adjr2 )
```

If we look at the plot after 9 curve flattens. So with best subset model we can choose either 9 or 10 variables.  Lets look at other variable selection methods:

# Forward and Backward

```{r}
reg.fit.fwd <- regsubsets(candidate~.,data = full.set,nvmax=10,method = "forward")

summary(reg.fit.fwd)

plot(reg.fit.fwd)
plot(reg.fit.fwd, scale = "adjr2")
```



Backward:

```{r}
reg.fit.bwd <- regsubsets(candidate~.,data = full.set,nvmax=20, method = "backward")

summary(reg.fit.bwd)

plot(reg.fit.bwd)
plot(reg.fit.bwd, scale = "adjr2")
```
 

Running correlation between variables to check for any multicollinearity: 

```{r}
corr <- full.set%>%
  select(anticipation:negative)%>%
  cor()

corrplot(cor(corr), method="color", type = "upper", tl.col="black",tl.srt=40, addCoef.col = "gray8", diag = T, number.cex = 0.65)
```


# Ridge and Lasso

Ridge regression shrinks the coeﬃcients by imposing a penalty on their size. The ridge coeﬃcients minimize a penalized residual sum of squares. Here λ≥0 is a complexity parameter that controls the amount of shrinkage: the larger the value of λ, the greater the amount of shrinkage. The coeﬃcients are shrunk towards zero (and towards each other).

By penalizing the RSS we try to avoid that highly correlated regressors cancel each other. An especially large positive coeﬃcient β can be canceled by a similarly large negative coeﬃcient β. By imposing a size constraint on the coeﬃcients this phenomenon can be prevented.

It can be shown that PCR is very similar to ridge regression: both methods use the principal components of the input matrix X. Ridge regression shrinks the coeﬃcients of the principal components, the shrinkage depends on the corresponding eigenvalues; PCR completely discards the components to the smallest p−q eigenvalues.

The lasso is a shrinkage method like ridge, but the L1 norm rather than the L2 norm is used in the constraints. L1-norm loss function is also known as least absolute deviations (LAD), least absolute errors (LAE). It is basically minimizing the sum of the absolute differences between the target value and the estimated values. L2-norm loss function is also known as least squares error (LSE). It is basically minimizing the sum of the square of the differences between the target value (Yi) and the estimated values. The difference between the L1 and L2 is just that L2 is the sum of the square of the weights, while L1 is just the sum of the weights. L1-norm tends to produces sparse coefficients and has Built-in feature selection. L1-norm does not have an analytical solution, but L2-norm does. This allows the L2-norm solutions to be calculated computationally efficiently. L2-norm has unique solutions while L1-norm does not.

```{r}
library(glmnet)
X= model.matrix(candidate~.,data=full.set)
Y=candidate
lasso = glmnet(X,Y, alpha =1, family = "binomial",lambda = seq(0,10,0.01))
plot(lasso)
```
Perform 10 fold validation and find the optimum lambda

```{r}
cv.lasso = cv.glmnet(X,candidate,alpha=1,family = "binomial",nfolds = 10, lambda= seq(0,10,0.1))

plot(cv.lasso)
```




```{r}
cv.lasso$lambda.min
```

Perform validation set approach to compute test MSE

```{r}
n= length(candidate)

Z = sample(n,n/2)

lasso = glmnet(X[Z,],candidate[Z], alpha=1, family = "binomial",lambda = seq(0,10,0.01))
Yhat = predict(lasso,cv.lasso$lambda.min, newx=X[-Z,])

mean((Yhat-candidate[-Z])^2)
```


***
## GLM binomial Sentiment:

```{r}
set.seed(1)
n= length(candidate)

Z = sample(1:nrow(full.set), 0.7*nrow(full.set))
glm.fit <- glm(candidate ~ ., data=full.set[Z,],family=binomial) 

summary(glm.fit)

# dev.Rsq <- (null.dev - res.dev)/null.dev
# dev.Rsq <- (27100 -25706)/27100
# dev.Rsq
```


```{r}
Probability = predict(glm.fit,full.set[-Z,], type="response")
Predicted.Direction = rep("0",length(Probability))
Predicted.Direction[ Probability > 0.5 ] = "1"

table( candidate[-Z], Predicted.Direction )


mean( candidate[-Z] == Predicted.Direction )


```

We correctly classified 2599 for Biden and 2410 for Trump. overall our correct classification rate is 59% and error rate is 40% - a little better than coin toss. 




## Classification Tree
```{r}
library(tree)
class.tree <- tree(candidate ~., data=full.set, mindev=0.005)

summary(class.tree)

plot(class.tree,type ="uniform")
text(class.tree, pretty=0)
```


Process above may produce good predictions on training data but is likely to overfit because we might grow a large tree. In this case reduce it by eliminating the least important nodes.


# estimate the correct classification rate by cross validation. 
```{r}
set.seed(1)

n= length(candidate)

Z = sample(1:nrow(full.set), 0.7*nrow(full.set))

train.tree = tree(candidate ~ .,data=full.set[Z,])

candidate.predict = predict(train.tree,full.set, type = "class")
table(candidate.predict[-Z], candidate[-Z])
mean(candidate.predict[-Z]!=candidate[-Z])


# Using cross validation to determine the optimal complexity of a tree and the number of terminal nodes that minimizes the deviance. 
cv = cv.tree(train.tree)
cv
plot(cv)


# instead of optimizing by the smallest deviance, optimize the complexity and the number of terminal nodes by the smallest mis-classification error
cv = cv.tree(train.tree, FUN = prune.misclass)
cv
plot(cv)

#prune the tree to the optimal size which is 3 obtained above
tree.pruned = prune.misclass(train.tree, best = 3)

plot(tree.pruned)
text(tree.pruned, pretty=0)
```


Classification tree returned : positive as the most important predictor - surprise being the next important predictor. When the tweet message is  positive and it is greater than 2.5 - we will be predicting the message is more for Biden. 

When message is positive but less than 2.5 and greater than 0.5 surprise ( in another word, when the message is not very positive but has some surprise elements in it than we classify the message as more support for Biden.)

Again, this is great for interpretation but if we look at the test MSE = 0.4, we are still doing a just a little better than tossing a coin. classification tree returned similar results as the logistic regression. 



Above 2 methods provided great intrepretability but prediction accuracy was low. our goal was to have a high prediction accuracy. Therefore,we opted run some models where we lose some intrepretability but we will gain some prediction accuracy


# Random Forrest and Bagging

***
```{r}
library(randomForest)
set.seed(1)
train <- sample(1:nrow(full.set), 0.7*nrow(full.set)) 
# Bagging 500 trees (default), with 13 predictors each, with variable importance statistics
bag.election <- randomForest(candidate~., data=full.set[train,], mtry=10, importance=T)
# mtry =30 uses all 30 predictors; ntree = is number of trees to fit (default is 500 trees)
bag.election # Check it out
varImpPlot(bag.election) # Variable importance plots
importance(bag.election)

# Bagging Tree Predictions
bag.pred <- predict(bag.election, newdata=full.set[-train,]) # Predict with the train model and test data
plot(bag.pred, full.set$candidate[-train], xlab="Predicted", ylab="Actual") # Plot predicted vs. actual
abline(0,1) # Draw a 45 degree line (intercept=0; slope=1)
mean((bag.pred-full.set$candidate[-train])^2) # Get the mean squared error -> 0.1373493


# Random Forest
set.seed(1)
train <- sample(1:nrow(full.set), 0.7*nrow(full.set))
# Bagging 500 trees (default), with 6 predictors each, with variable importance statistics
rf.election <- randomForest(candidate ~ . , data=full.set[-train,], mtry=10, importance=T)
plot(rf.election) # Looks like the MSE error flattens after about 80 trees

rf.election # Check out the tree
varImpPlot(rf.election) # We can also plot the results
importance(rf.election) # To view the importance of each variable

rf.pred <- predict(rf.election,newdata=full.set[-train,]) # Predict with the train model and test data
plot(rf.pred , full.set$candidate[-train], xlab="Predicted", ylab="Actual") # Plot predicted vs. actual 
abline(0,1) # Draw a 45 degree line  (intercept=0; slope=1)
mean((rf.pred-full.set$candidate[-train])^2) # Mean squared error is much lower than Bagging -> 0.1093216
```






*** 

Minh Analysis
```{r}
# Read Dataset
full.set <- read.csv("Data/sent_training.csv")

model.set <- full.set %>%
  rename(c("biden"="candidate")) %>%
  mutate(biden = ifelse(biden=="Biden", 1, 0)) %>%
  select(-date, -name, -sources, -isretweeted, -length)

# Split training and testing sets - set.seed(1)

# set.seed(1)
train <- sample(1:nrow(model.set), 0.7*nrow(model.set)) 
# Bagging 500 trees (default), with 13 predictors each, with variable importance statistics
bag.election <- randomForest(biden~., data=model.set[train,], mtry=30, importance=T)
# mtry =30 uses all 30 predictors; ntree = is number of trees to fit (default is 500 trees)
bag.election # Check it out
varImpPlot(bag.election) # Variable importance plots
importance(bag.election)

# Bagging Tree Predictions
bag.pred <- predict(bag.election, newdata=model.set[-train,]) # Predict with the train model and test data
plot(bag.pred, model.set$biden[-train], xlab="Predicted", ylab="Actual") # Plot predicted vs. actual
abline(0,1) # Draw a 45 degree line (intercept=0; slope=1)
mean((bag.pred-model.set$biden[-train])^2) # Get the mean squared error -> 0.1373493


# Random Forest
set.seed(1)
train <- sample(1:nrow(full.set), 0.7*nrow(full.set))
# Bagging 500 trees (default), with 6 predictors each, with variable importance statistics
rf.election <- randomForest(candidate ~ . , data=full.set[-train,], mtry=6, importance=T)
plot(rf.election) # Looks like the MSE error flattens after about 80 trees

rf.election # Check out the tree
varImpPlot(rf.election) # We can also plot the results
importance(rf.election) # To view the importance of each variable

rf.pred <- predict(rf.election,newdata=full.set[-train,]) # Predict with the train model and test data
plot(rf.pred , full.set$candidate[-train], xlab="Predicted", ylab="Actual") # Plot predicted vs. actual 
abline(0,1) # Draw a 45 degree line  (intercept=0; slope=1)
mean((rf.pred-full.set$candidate[-train])^2) # Mean squared error is much lower than Bagging -> 0.1093216
```



